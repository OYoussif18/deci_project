<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>WeRateDogs Data Wrangling Report</title>
</head>
<body>

<h1>WeRateDogs Data Wrangling Report</h1>

<h2>Introduction</h2>
<p>
This report summarizes the data wrangling process performed on the WeRateDogs Twitter dataset. The objective was to gather data from multiple sources, assess quality and tidiness issues, clean the data appropriately, and store it for future analysis. The final dataset combines tweet metadata, image predictions, and tweet engagement metrics to allow deeper insights into dog ratings and social engagement trends.
</p>

<h2>Data Gathering</h2>
<p>
Three datasets were collected using different methods:
</p>
<ul>
  <li><strong>Enhanced Twitter Archive</strong> – A CSV file manually downloaded (`twitter-archive-enhanced.csv`) containing tweet metadata including dog ratings and names.</li>
  <li><strong>Image Predictions</strong> – A TSV file (`image-predictions.tsv`) downloaded using Python's `requests` library. It contains top three breed predictions per tweet image along with confidence values.</li>
  <li><strong>Tweet Statistics</strong> – A JSON file (`tweet-json.txt`) generated by querying Twitter's API for each tweet ID, capturing retweet and favorite counts. Each line was a separate JSON object.</li>
</ul>

<h2>Data Assessment</h2>
<p>
Each dataset was assessed visually and programmatically. Key issues identified included:
</p>
<ul>
  <li>Invalid dog names such as "a", "an", or lowercase words.</li>
  <li>Outliers in ratings (e.g., values above 20).</li>
  <li>Predictions with `p1_conf` > 1, which are invalid.</li>
  <li>Predictions that did not describe dogs (e.g., objects or other animals).</li>
  <li>Missing values in columns like `name` and some `retweet_count` values.</li>
  <li>Duplicate rows across datasets.</li>
</ul>

<h2>Data Cleaning</h2>
<p>
To clean the data:
</p>
<ul>
  <li><strong>Duplicates</strong> were removed using `drop_duplicates()`.</li>
  <li><strong>Rows with missing or invalid `tweet_id`, prediction data, or ratings</strong> were dropped.</li>
  <li><strong>Predictions</strong> were filtered to keep only rows where `p1_dog == True` and `p1_conf > 0.2`.</li>
  <li><strong>Dog names</strong> like "a", "the", and "an" were replaced with `NaN`.</li>
  <li><strong>Ratings</strong> were capped at a maximum of 10 to remove exaggerations.</li>
  <li><strong>Timestamps</strong> were converted to datetime format using pandas with timezone awareness (`datetime64[ns, UTC]`).</li>
  <li><strong>Unnecessary columns</strong> were dropped to improve dataset clarity and reduce redundancy.</li>
</ul>

<h2>Data Storage</h2>
<p>
The cleaned datasets were merged into one master dataset using the `tweet_id` as a key. Relevant columns retained include:
</p>
<ul>
  <li>Tweet ID</li>
  <li>Predicted breed (`dog_type`)</li>
  <li>Cleaned dog name (`name`)</li>
  <li>Rating (`rating`)</li>
  <li>Tweet timestamp</li>
  <li>Engagement metrics (`retweet_count` and `favorite_count`)</li>
</ul>
<p>
The final clean dataset was saved as <strong>`twitter_archive_master.csv`</strong> for further analysis.
</p>

<h2>Conclusion</h2>
<p>
The wrangling process successfully resolved quality and tidiness issues across the three datasets. The final dataset combines breed prediction, tweet metadata, and user engagement in a clean, structured format that enables meaningful insights into rating trends and breed popularity on social media.
</p>

</body>
</html>
